{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the minibatcher for batch learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def minibatcher(data_x, data_y, batch_size, num_repeats):\n",
    "    assert(data_x.shape[0])\n",
    "    data_size = data_x.shape[0]\n",
    "    for _ in range(num_repeats):\n",
    "        start = 0\n",
    "        while start < data_size:\n",
    "            yield data_x[start:start + batch_size], data_y[start:start + batch_size]\n",
    "            start += batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicate all of the training constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classes = [\"walking\", \"sitting\", \"table\", \"stairs\", \"car\"]\n",
    "\n",
    "n_hidden = 100 # Size of the LSTM hidden layer\n",
    "batch_size = 8 # Number of data points in a batch\n",
    "learning_rate = 0.01 # Learning rate of the optimizer\n",
    "dropout_keep_prob = .8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data, and split it into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_data = np.load(\"gmail/clean_data.npy\")\n",
    "data_labels = np.load(\"gmail/labels.npy\")\n",
    "\n",
    "# Update the dataset to only be the labeled data (the ones that aren't 0)\n",
    "labeled = data_labels != 0\n",
    "input_motion_data = clean_data[labeled]\n",
    "output_motion_data = data_labels[labeled] - 1 # Need to decrement by 1 since we removed all the 0s\n",
    "n_samples = input_motion_data.shape[0]\n",
    "n_steps = input_motion_data.shape[1]\n",
    "n_input = input_motion_data.shape[2]\n",
    "n_classes = np.max(output_motion_data) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_classes = np.zeros((n_samples, n_classes))\n",
    "for i in range(n_samples):\n",
    "    output_classes[i, output_motion_data[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(input_motion_data, output_classes, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('walking', 54), ('sitting', 48), ('table', 68), ('stairs', 20), ('car', 78)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip(classes, np.bincount(np.argmax(Y_train, axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the placeholders and variables to be optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "# Define weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden, n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def RNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, n_steps, n_input)\n",
    "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
    "    \n",
    "    # Permuting batch_size and n_steps\n",
    "    x = tf.transpose(x, [1, 0, 2])\n",
    "    # Reshaping to (n_steps*batch_size, n_input)\n",
    "    x = tf.reshape(x, [-1, n_input])\n",
    "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
    "    x = tf.split(0, n_steps, x)\n",
    "\n",
    "    # Define a lstm cell with tensorflow\n",
    "    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
    "\n",
    "    # Get lstm cell output\n",
    "    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
    "\n",
    "    hidden_layer = tf.nn.relu(tf.matmul(outputs[-1], weights['hidden']) + biases['hidden'])\n",
    "    hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
    "    return tf.matmul(hidden_layer, weights['out']) + biases['out']\n",
    "    \n",
    "    \n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    # return tf.matmul(outputs[-1], weights['out']) + biases['out']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the various graphs: notably cost, optimizer, and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = RNN(x, weights, biases)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0, Minibatch Loss= 17.240484, Training Accuracy= 0.60000, Test Accuracy= 0.20896\n",
      "Batch 5, Minibatch Loss= 5.334779, Training Accuracy= 0.80000, Test Accuracy= 0.62687\n",
      "Batch 10, Minibatch Loss= 6.588993, Training Accuracy= 0.90000, Test Accuracy= 0.64179\n",
      "Batch 15, Minibatch Loss= 1.040450, Training Accuracy= 0.80000, Test Accuracy= 0.65672\n",
      "Batch 20, Minibatch Loss= 0.000001, Training Accuracy= 0.90000, Test Accuracy= 0.76119\n",
      "Batch 25, Minibatch Loss= 3.497866, Training Accuracy= 0.70000, Test Accuracy= 0.77612\n",
      "Batch 30, Minibatch Loss= 9.008074, Training Accuracy= 0.60000, Test Accuracy= 0.73134\n",
      "Batch 35, Minibatch Loss= 7.065759, Training Accuracy= 0.50000, Test Accuracy= 0.68657\n",
      "Batch 40, Minibatch Loss= 1.483553, Training Accuracy= 0.90000, Test Accuracy= 0.67164\n",
      "Batch 45, Minibatch Loss= 4.219509, Training Accuracy= 0.70000, Test Accuracy= 0.73134\n",
      "Batch 50, Minibatch Loss= 1.327806, Training Accuracy= 0.90000, Test Accuracy= 0.73134\n",
      "Batch 55, Minibatch Loss= 0.903301, Training Accuracy= 0.80000, Test Accuracy= 0.70149\n",
      "Batch 60, Minibatch Loss= 2.871228, Training Accuracy= 0.70000, Test Accuracy= 0.73134\n",
      "Batch 65, Minibatch Loss= 2.436831, Training Accuracy= 0.90000, Test Accuracy= 0.71642\n",
      "Batch 70, Minibatch Loss= 0.100203, Training Accuracy= 1.00000, Test Accuracy= 0.68657\n",
      "Batch 75, Minibatch Loss= 2.674092, Training Accuracy= 0.90000, Test Accuracy= 0.77612\n",
      "Batch 80, Minibatch Loss= 0.056551, Training Accuracy= 1.00000, Test Accuracy= 0.79104\n",
      "Batch 85, Minibatch Loss= 0.532845, Training Accuracy= 0.90000, Test Accuracy= 0.80597\n",
      "Batch 90, Minibatch Loss= 2.228567, Training Accuracy= 1.00000, Test Accuracy= 0.76119\n",
      "Batch 95, Minibatch Loss= 0.398450, Training Accuracy= 1.00000, Test Accuracy= 0.76119\n",
      "Batch 100, Minibatch Loss= 7.573659, Training Accuracy= 0.80000, Test Accuracy= 0.76119\n",
      "Batch 105, Minibatch Loss= 3.480837, Training Accuracy= 0.90000, Test Accuracy= 0.80597\n",
      "Batch 110, Minibatch Loss= 6.058356, Training Accuracy= 1.00000, Test Accuracy= 0.74627\n",
      "Batch 115, Minibatch Loss= 1.481454, Training Accuracy= 0.90000, Test Accuracy= 0.73134\n",
      "Batch 120, Minibatch Loss= 0.620774, Training Accuracy= 1.00000, Test Accuracy= 0.73134\n",
      "Batch 125, Minibatch Loss= 3.436855, Training Accuracy= 0.90000, Test Accuracy= 0.73134\n",
      "Batch 130, Minibatch Loss= 0.243423, Training Accuracy= 0.70000, Test Accuracy= 0.71642\n",
      "Batch 135, Minibatch Loss= 2.707163, Training Accuracy= 0.90000, Test Accuracy= 0.76119\n",
      "Batch 140, Minibatch Loss= 0.145613, Training Accuracy= 1.00000, Test Accuracy= 0.73134\n",
      "Batch 145, Minibatch Loss= 1.179402, Training Accuracy= 1.00000, Test Accuracy= 0.76119\n",
      "Batch 150, Minibatch Loss= 0.100565, Training Accuracy= 0.90000, Test Accuracy= 0.77612\n",
      "Batch 155, Minibatch Loss= 1.809484, Training Accuracy= 0.90000, Test Accuracy= 0.76119\n",
      "Batch 160, Minibatch Loss= 0.004111, Training Accuracy= 0.90000, Test Accuracy= 0.74627\n",
      "Batch 165, Minibatch Loss= 2.764364, Training Accuracy= 0.90000, Test Accuracy= 0.74627\n",
      "Batch 170, Minibatch Loss= 3.038530, Training Accuracy= 0.80000, Test Accuracy= 0.73134\n",
      "Batch 175, Minibatch Loss= 0.612806, Training Accuracy= 1.00000, Test Accuracy= 0.79104\n",
      "Batch 180, Minibatch Loss= 0.741797, Training Accuracy= 1.00000, Test Accuracy= 0.76119\n",
      "Batch 185, Minibatch Loss= 1.251836, Training Accuracy= 0.80000, Test Accuracy= 0.74627\n",
      "Batch 190, Minibatch Loss= 1.543731, Training Accuracy= 0.80000, Test Accuracy= 0.71642\n",
      "Batch 195, Minibatch Loss= 1.555031, Training Accuracy= 0.80000, Test Accuracy= 0.73134\n",
      "Batch 200, Minibatch Loss= 0.000012, Training Accuracy= 1.00000, Test Accuracy= 0.76119\n",
      "Batch 205, Minibatch Loss= 0.007406, Training Accuracy= 1.00000, Test Accuracy= 0.74627\n",
      "Batch 210, Minibatch Loss= 0.000673, Training Accuracy= 1.00000, Test Accuracy= 0.73134\n",
      "Batch 215, Minibatch Loss= 0.000003, Training Accuracy= 1.00000, Test Accuracy= 0.76119\n",
      "Batch 220, Minibatch Loss= 0.213648, Training Accuracy= 1.00000, Test Accuracy= 0.71642\n",
      "Batch 225, Minibatch Loss= 0.000013, Training Accuracy= 1.00000, Test Accuracy= 0.70149\n",
      "Batch 230, Minibatch Loss= 0.000000, Training Accuracy= 1.00000, Test Accuracy= 0.68657\n",
      "Batch 235, Minibatch Loss= 0.296537, Training Accuracy= 1.00000, Test Accuracy= 0.67164\n",
      "Batch 240, Minibatch Loss= 0.000011, Training Accuracy= 1.00000, Test Accuracy= 0.67164\n",
      "Batch 245, Minibatch Loss= 0.437231, Training Accuracy= 0.90000, Test Accuracy= 0.65672\n",
      "Batch 250, Minibatch Loss= 0.000357, Training Accuracy= 1.00000, Test Accuracy= 0.67164\n",
      "Batch 255, Minibatch Loss= 0.000004, Training Accuracy= 1.00000, Test Accuracy= 0.68657\n",
      "Batch 260, Minibatch Loss= 3.725387, Training Accuracy= 1.00000, Test Accuracy= 0.68657\n",
      "Batch 265, Minibatch Loss= 1.827926, Training Accuracy= 0.90000, Test Accuracy= 0.70149\n",
      "Batch 270, Minibatch Loss= 1.747864, Training Accuracy= 0.80000, Test Accuracy= 0.73134\n",
      "Batch 275, Minibatch Loss= 1.266962, Training Accuracy= 0.90000, Test Accuracy= 0.74627\n",
      "Batch 280, Minibatch Loss= 0.725491, Training Accuracy= 0.80000, Test Accuracy= 0.71642\n",
      "Batch 285, Minibatch Loss= 0.042143, Training Accuracy= 0.80000, Test Accuracy= 0.73134\n",
      "Batch 290, Minibatch Loss= 1.927448, Training Accuracy= 0.90000, Test Accuracy= 0.74627\n",
      "Batch 295, Minibatch Loss= 1.091694, Training Accuracy= 1.00000, Test Accuracy= 0.74627\n",
      "Batch 300, Minibatch Loss= 0.134872, Training Accuracy= 0.90000, Test Accuracy= 0.71642\n",
      "Batch 305, Minibatch Loss= 0.593323, Training Accuracy= 0.90000, Test Accuracy= 0.70149\n",
      "Batch 310, Minibatch Loss= 0.000723, Training Accuracy= 1.00000, Test Accuracy= 0.73134\n",
      "Batch 315, Minibatch Loss= 2.251941, Training Accuracy= 0.90000, Test Accuracy= 0.73134\n",
      "Batch 320, Minibatch Loss= 0.000184, Training Accuracy= 1.00000, Test Accuracy= 0.73134\n",
      "Batch 325, Minibatch Loss= 0.009963, Training Accuracy= 1.00000, Test Accuracy= 0.74627\n",
      "Batch 330, Minibatch Loss= 0.238780, Training Accuracy= 1.00000, Test Accuracy= 0.76119\n",
      "Batch 335, Minibatch Loss= 3.065436, Training Accuracy= 0.80000, Test Accuracy= 0.74627\n",
      "Batch 340, Minibatch Loss= 0.745110, Training Accuracy= 1.00000, Test Accuracy= 0.73134\n",
      "Batch 345, Minibatch Loss= 0.429046, Training Accuracy= 1.00000, Test Accuracy= 0.74627\n",
      "Batch 350, Minibatch Loss= 0.000887, Training Accuracy= 1.00000, Test Accuracy= 0.73134\n",
      "Batch 355, Minibatch Loss= 0.000352, Training Accuracy= 1.00000, Test Accuracy= 0.71642\n",
      "Batch 360, Minibatch Loss= 0.730733, Training Accuracy= 0.90000, Test Accuracy= 0.71642\n",
      "Batch 365, Minibatch Loss= 0.857719, Training Accuracy= 1.00000, Test Accuracy= 0.74627\n",
      "Batch 370, Minibatch Loss= 1.770463, Training Accuracy= 0.90000, Test Accuracy= 0.77612\n",
      "Batch 375, Minibatch Loss= 0.000030, Training Accuracy= 1.00000, Test Accuracy= 0.76119\n",
      "Batch 380, Minibatch Loss= 0.002559, Training Accuracy= 1.00000, Test Accuracy= 0.74627\n",
      "Batch 385, Minibatch Loss= 0.088580, Training Accuracy= 1.00000, Test Accuracy= 0.74627\n",
      "Batch 390, Minibatch Loss= 0.000060, Training Accuracy= 1.00000, Test Accuracy= 0.71642\n",
      "Batch 395, Minibatch Loss= 0.809267, Training Accuracy= 0.80000, Test Accuracy= 0.70149\n",
      "Batch 400, Minibatch Loss= 0.001082, Training Accuracy= 1.00000, Test Accuracy= 0.74627\n",
      "Batch 405, Minibatch Loss= 0.392833, Training Accuracy= 1.00000, Test Accuracy= 0.73134\n",
      "Batch 410, Minibatch Loss= 0.002050, Training Accuracy= 1.00000, Test Accuracy= 0.73134\n",
      "Batch 415, Minibatch Loss= 0.001139, Training Accuracy= 1.00000, Test Accuracy= 0.71642\n",
      "Batch 420, Minibatch Loss= 1.518913, Training Accuracy= 0.90000, Test Accuracy= 0.70149\n",
      "Batch 425, Minibatch Loss= 0.004463, Training Accuracy= 1.00000, Test Accuracy= 0.73134\n",
      "Batch 430, Minibatch Loss= 0.000169, Training Accuracy= 1.00000, Test Accuracy= 0.71642\n",
      "Batch 435, Minibatch Loss= 0.000029, Training Accuracy= 1.00000, Test Accuracy= 0.71642\n",
      "Batch 440, Minibatch Loss= 0.004152, Training Accuracy= 1.00000, Test Accuracy= 0.73134\n",
      "Batch 445, Minibatch Loss= 0.000362, Training Accuracy= 1.00000, Test Accuracy= 0.73134\n",
      "Batch 450, Minibatch Loss= 0.000000, Training Accuracy= 1.00000, Test Accuracy= 0.74627\n",
      "Batch 455, Minibatch Loss= 0.070027, Training Accuracy= 1.00000, Test Accuracy= 0.73134\n",
      "Batch 460, Minibatch Loss= 0.896017, Training Accuracy= 1.00000, Test Accuracy= 0.70149\n",
      "Batch 465, Minibatch Loss= 0.714349, Training Accuracy= 0.80000, Test Accuracy= 0.70149\n",
      "Batch 470, Minibatch Loss= 0.000730, Training Accuracy= 1.00000, Test Accuracy= 0.71642\n",
      "Batch 475, Minibatch Loss= 0.363878, Training Accuracy= 0.90000, Test Accuracy= 0.71642\n",
      "Batch 480, Minibatch Loss= 1.417187, Training Accuracy= 1.00000, Test Accuracy= 0.65672\n",
      "Batch 485, Minibatch Loss= 0.000372, Training Accuracy= 1.00000, Test Accuracy= 0.67164\n",
      "Batch 490, Minibatch Loss= 0.014220, Training Accuracy= 1.00000, Test Accuracy= 0.68657\n",
      "Batch 495, Minibatch Loss= 1.400716, Training Accuracy= 1.00000, Test Accuracy= 0.71642\n",
      "Batch 500, Minibatch Loss= 0.010213, Training Accuracy= 1.00000, Test Accuracy= 0.67164\n",
      "Batch 505, Minibatch Loss= 0.025544, Training Accuracy= 1.00000, Test Accuracy= 0.67164\n",
      "Batch 510, Minibatch Loss= 0.000181, Training Accuracy= 1.00000, Test Accuracy= 0.68657\n",
      "Batch 515, Minibatch Loss= 0.000044, Training Accuracy= 1.00000, Test Accuracy= 0.71642\n",
      "Batch 520, Minibatch Loss= 0.001477, Training Accuracy= 1.00000, Test Accuracy= 0.71642\n",
      "Batch 525, Minibatch Loss= 0.170583, Training Accuracy= 0.90000, Test Accuracy= 0.71642\n",
      "Batch 530, Minibatch Loss= 0.004980, Training Accuracy= 1.00000, Test Accuracy= 0.70149\n",
      "Batch 535, Minibatch Loss= 0.001839, Training Accuracy= 1.00000, Test Accuracy= 0.68657\n",
      "Final Test accuracy = 0.70149\n"
     ]
    }
   ],
   "source": [
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for (rep, (batch_x, batch_y)) in enumerate(minibatcher(X_train,Y_train,10, 20)):\n",
    "    sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout_keep_prob})\n",
    "    if rep % 5 == 0:\n",
    "        # Calculate batch accuracy\n",
    "        acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        test_acc = sess.run(accuracy, feed_dict={x: X_test, y: Y_test, keep_prob: 1.0})\n",
    "        # Calculate batch loss\n",
    "        loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout_keep_prob})\n",
    "        print \"Batch \" + str(rep) + \", Minibatch Loss= \" + \\\n",
    "              \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "              \"{:.5f}\".format(acc) + \", Test Accuracy= \" + \"{:.5f}\".format(test_acc)\n",
    "final_test_acc = sess.run(accuracy, feed_dict={x: X_test, y: Y_test, keep_prob: 1.0})\n",
    "print \"Final Test accuracy = \" + \"{:.5f}\".format(final_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count which classes were confused "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('walking', 'car'): 9\n",
      "('sitting', 'car'): 6\n",
      "('walking', 'stairs'): 3\n",
      "('walking', 'sitting'): 1\n",
      "('stairs', 'car'): 1\n"
     ]
    }
   ],
   "source": [
    "# Get indices of incorrect predictions in the test set\n",
    "test_predicted = np.argmax(sess.run(pred, feed_dict={x: X_test, keep_prob: 1.0}), axis=1)\n",
    "test_actual = np.argmax(Y_test, axis=1)\n",
    "wrong_predictions = test_predicted != test_actual\n",
    "\n",
    "mistakes = zip(test_predicted[wrong_prediction], test_actual[wrong_predictions])\n",
    "# Sort the predicted/expected so that mistaking class 1 for class 3 is the same \n",
    "# as mistaking class 3 for class 1, for example\n",
    "mistakes = sorted(map(lambda x: sorted(x), mistakes))\n",
    "# Convert classs number to class names\n",
    "mistakes = map(lambda p: (classes[p[0]], classes[p[1]]), mistakes)\n",
    "\n",
    "from collections import Counter\n",
    "for m, n in Counter(mistakes).most_common():\n",
    "    print \"%s: %d\" % (str(m), n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify the Unlabelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walking: 14\n",
      "sitting: 97\n",
      "table: 11\n",
      "stairs: 39\n",
      "car: 52\n"
     ]
    }
   ],
   "source": [
    "unlabeled = data_labels == 0\n",
    "input_motion_data = clean_data[unlabeled]\n",
    "\n",
    "Y = np.argmax(sess.run(pred, feed_dict={x: input_motion_data, keep_prob: 1.0}), axis=1)\n",
    "class_count = zip(classes, np.bincount(Y))\n",
    "\n",
    "for c in class_count:\n",
    "    print \"%s: %d\" % (c[0], c[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
