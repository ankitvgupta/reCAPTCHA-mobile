{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  },
  "name": "",
  "signature": "sha256:cc778aa7af8fbf027767815e16af13c61ea506913e5c346f85749450f56c1c1a"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import tensorflow as tf\n",
      "from tensorflow.python.ops import rnn, rnn_cell\n",
      "from sklearn.model_selection import train_test_split"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Set up the minibatcher for batch learning"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def minibatcher(data_x, data_y, batch_size, num_repeats):\n",
      "    assert(data_x.shape[0])\n",
      "    data_size = data_x.shape[0]\n",
      "    for _ in range(num_repeats):\n",
      "        start = 0\n",
      "        while start < data_size:\n",
      "            yield data_x[start:start + batch_size], data_y[start:start + batch_size]\n",
      "            start += batch_size\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Indicate all of the training constants"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "classes = [\"walking\", \"sitting\", \"table\", \"stairs\", \"car\"]\n",
      "\n",
      "n_hidden = 100 # Size of the LSTM hidden layer\n",
      "batch_size = 8 # Number of data points in a batch\n",
      "learning_rate = 0.01 # Learning rate of the optimizer\n",
      "dropout_keep_prob = .8\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Import the data, and split it into training and testing sets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean_data = np.load(\"gmail/clean_data.npy\")\n",
      "data_labels = np.load(\"gmail/labels.npy\")\n",
      "\n",
      "# Update the dataset to only be the labeled data (the ones that aren't 0)\n",
      "labeled = data_labels != 0\n",
      "input_motion_data = clean_data[labeled]\n",
      "output_motion_data = data_labels[labeled] - 1 # Need to decrement by 1 since we removed all the 0s\n",
      "n_samples = input_motion_data.shape[0]\n",
      "n_steps = input_motion_data.shape[1]\n",
      "n_input = input_motion_data.shape[2]\n",
      "n_classes = np.max(output_motion_data) + 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "output_classes = np.zeros((n_samples, n_classes))\n",
      "for i in range(n_samples):\n",
      "    output_classes[i, output_motion_data[i]] = 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train_unfiltered, X_test, Y_train_unfiltered, Y_test = train_test_split(input_motion_data, output_classes, test_size=.2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Count of each class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "counts_per_class = np.bincount(np.argmax(Y_train_unfiltered, axis=1))\n",
      "smallest_class_size = min(counts_per_class)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'Y_train' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-7-cdf6560ae9ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcounts_per_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbincount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msmallest_class_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcounts_per_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'Y_train' is not defined"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = np.zeros((smallest_class_size*n_classes, X_train_unfiltered.shape[1], X_train_unfiltered.shape[2]))\n",
      "Y_train = np.zeros((smallest_class_size*n_classes, Y_train_unfiltered.shape[1]))\n",
      "\n",
      "num_in_class = np.zeros(n_classes)\n",
      "loc = 0\n",
      "for x, y in zip(X_train_unfiltered, Y_train_unfiltered):\n",
      "    class_of_sample = np.argmax(y)\n",
      "    if num_in_class[class_of_sample] >= smallest_class_size:\n",
      "        continue\n",
      "    X_train[loc] = x\n",
      "    Y_train[loc] = y\n",
      "    num_in_class[class_of_sample] += 1\n",
      "    loc += 1\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "zip(classes, np.bincount(np.argmax(Y_train, axis=1)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Define the placeholders and variables to be optimized"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
      "y = tf.placeholder(\"float\", [None, n_classes])\n",
      "keep_prob = tf.placeholder(tf.float32)\n",
      "# Define weights\n",
      "weights = {\n",
      "    'hidden': tf.Variable(tf.random_normal([n_hidden, n_hidden])),\n",
      "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
      "}\n",
      "biases = {\n",
      "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
      "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Define the model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def RNN(x, weights, biases):\n",
      "\n",
      "    # Prepare data shape to match `rnn` function requirements\n",
      "    # Current data input shape: (batch_size, n_steps, n_input)\n",
      "    # Required shape: 'n_steps' tensors list of shape (batch_size, n_input)\n",
      "    \n",
      "    # Permuting batch_size and n_steps\n",
      "    x = tf.transpose(x, [1, 0, 2])\n",
      "    # Reshaping to (n_steps*batch_size, n_input)\n",
      "    x = tf.reshape(x, [-1, n_input])\n",
      "    # Split to get a list of 'n_steps' tensors of shape (batch_size, n_input)\n",
      "    x = tf.split(0, n_steps, x)\n",
      "\n",
      "    # Define a lstm cell with tensorflow\n",
      "    lstm_cell = rnn_cell.BasicLSTMCell(n_hidden, forget_bias=1.0)\n",
      "\n",
      "    # Get lstm cell output\n",
      "    outputs, states = rnn.rnn(lstm_cell, x, dtype=tf.float32)\n",
      "\n",
      "    hidden_layer = tf.nn.relu(tf.matmul(outputs[-1], weights['hidden']) + biases['hidden'])\n",
      "    hidden_layer = tf.nn.dropout(hidden_layer, keep_prob)\n",
      "    return tf.matmul(hidden_layer, weights['out']) + biases['out']\n",
      "    \n",
      "    \n",
      "    # Linear activation, using rnn inner loop last output\n",
      "    # return tf.matmul(outputs[-1], weights['out']) + biases['out']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Define the various graphs: notably cost, optimizer, and accuracy"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pred = RNN(x, weights, biases)\n",
      "\n",
      "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
      "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
      "\n",
      "# Evaluate model\n",
      "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
      "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Run the optimization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sess = tf.Session()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sess.run(tf.initialize_all_variables())\n",
      "\n",
      "for (rep, (batch_x, batch_y)) in enumerate(minibatcher(X_train,Y_train,10, 20)):\n",
      "    sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout_keep_prob})\n",
      "    if rep % 5 == 0:\n",
      "        # Calculate batch accuracy\n",
      "        acc = sess.run(accuracy, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
      "        test_acc = sess.run(accuracy, feed_dict={x: X_test, y: Y_test, keep_prob: 1.0})\n",
      "        # Calculate batch loss\n",
      "        loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout_keep_prob})\n",
      "        print \"Batch \" + str(rep) + \", Minibatch Loss= \" + \\\n",
      "              \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
      "              \"{:.5f}\".format(acc) + \", Test Accuracy= \" + \"{:.5f}\".format(test_acc)\n",
      "final_test_acc = sess.run(accuracy, feed_dict={x: X_test, y: Y_test, keep_prob: 1.0})\n",
      "print \"Final Test accuracy = \" + \"{:.5f}\".format(final_test_acc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Count which classes were confused "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Get indices of incorrect predictions in the test set\n",
      "test_predicted = np.argmax(sess.run(pred, feed_dict={x: X_test, keep_prob: 1.0}), axis=1)\n",
      "test_actual = np.argmax(Y_test, axis=1)\n",
      "wrong_predictions = test_predicted != test_actual\n",
      "\n",
      "mistakes = zip(test_predicted[wrong_prediction], test_actual[wrong_predictions])\n",
      "# Sort the predicted/expected so that mistaking class 1 for class 3 is the same \n",
      "# as mistaking class 3 for class 1, for example\n",
      "mistakes = sorted(map(lambda x: sorted(x), mistakes))\n",
      "# Convert classs number to class names\n",
      "mistakes = map(lambda p: (classes[p[0]], classes[p[1]]), mistakes)\n",
      "\n",
      "from collections import Counter\n",
      "for m, n in Counter(mistakes).most_common():\n",
      "    print \"%s: %d\" % (str(m), n)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Classify the Unlabelled data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "unlabeled = data_labels == 0\n",
      "input_motion_data = clean_data[unlabeled]\n",
      "\n",
      "Y = np.argmax(sess.run(pred, feed_dict={x: input_motion_data, keep_prob: 1.0}), axis=1)\n",
      "class_count = zip(classes, np.bincount(Y))\n",
      "\n",
      "for c in class_count:\n",
      "    print \"%s: %d\" % (c[0], c[1])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}